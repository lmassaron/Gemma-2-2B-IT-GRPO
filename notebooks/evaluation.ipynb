{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba5ac989-ba06-4d17-a1af-b3fe691c4843",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['HF_HUB_ENABLE_HF_TRANSFER'] = '0'\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab6a3be6-971c-4ee1-bf6e-674ba525140b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import gc\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41846f8f-d1a5-42e4-a25f-033c2651f821",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "def extract_hash_answer(text: str) -> str | None:\n",
    "    if \"####\" not in text:\n",
    "        return None\n",
    "    return text.split(\"####\")[1].strip()\n",
    "\n",
    "def get_gsm8k_questions(split = \"test\") -> Dataset:\n",
    "    data = load_dataset('openai/gsm8k', 'main')[split] \n",
    "    data = data.map(lambda x: {\n",
    "        'prompt': [\n",
    "            {'role': 'user', 'content': x['question']},\n",
    "        ],\n",
    "        'answer': extract_hash_answer(x['answer'])\n",
    "    }) \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e510f899-30b4-4cd7-a15e-dd9a81881174",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = get_gsm8k_questions(split = \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85d8d5d1-780f-4c0e-aa60-caa647d3ef16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-12 00:53:34 __init__.py:190] Automatically detected platform cuda.\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import random\n",
    "from vllm import SamplingParams\n",
    "\n",
    "def count_xml(text) -> float:\n",
    "    count = 0.0\n",
    "    if text.count(\"<reasoning>\") == 1:\n",
    "        count += 0.125\n",
    "    if text.count(\"</reasoning>\") == 1:\n",
    "        count += 0.125\n",
    "    if text.count(\"<answer>\") == 1:\n",
    "        count += 0.125\n",
    "        count -= len(text.split(\"<answer>\")[-1].split(\"</answer>\")[0])*0.001\n",
    "    if text.count(\"</answer>\") == 1:\n",
    "        count += 0.125\n",
    "        count -= (len(text.split(\"</answer>\")[-1]) - 1)*0.001\n",
    "    return count\n",
    "    \n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "def evaluate_model(model, tokenizer, dataset, runs=3, temperature=0.8, top_p=0.95, max_tokens=1024, seed=42):\n",
    "    random.seed(seed)\n",
    "    correct_answer = 0\n",
    "    correct_format = 0\n",
    "    \n",
    "    for k, item in enumerate(tqdm(dataset)):\n",
    "        prompt = item['prompt']\n",
    "        answer = item['answer']\n",
    "    \n",
    "        text = tokenizer.apply_chat_template(prompt, tokenize = False, add_generation_prompt = True)\n",
    "    \n",
    "        sampling_params = SamplingParams(\n",
    "            temperature = temperature,\n",
    "            top_p = top_p,\n",
    "            max_tokens = max_tokens,\n",
    "        )\n",
    "\n",
    "        for _ in range(runs):\n",
    "            output = model.fast_generate(\n",
    "                [text],\n",
    "                sampling_params = sampling_params,\n",
    "                lora_request = None,\n",
    "                 use_tqdm=False,\n",
    "            )[0].outputs[0].text\n",
    "        \n",
    "            correct_answer += int(answer in output)\n",
    "            correct_format += count_xml(output)\n",
    "\n",
    "    scaling = 1 / (len(dataset) * runs)\n",
    "    answer_accuracy = correct_answer * scaling\n",
    "    format_accuracy = correct_format * scaling\n",
    "\n",
    "    print(f\"Percentage of correct answers: {answer_accuracy:.3f}\")\n",
    "    print(f\"Score of correct formats: {format_accuracy:.3f}\")\n",
    "\n",
    "    return {\n",
    "        \"answer_accuracy\": answer_accuracy,\n",
    "        \"format_accuracy\": format_accuracy,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e003c66-94b2-412b-807d-8749bb0d8e38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b8f7534c-07b3-4bdd-8500-ced250b22638",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.2.5: Fast Gemma2 patching. Transformers: 4.48.3.\n",
      "   \\\\   /|    GPU: NVIDIA GeForce RTX 3090. Max memory: 23.684 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.1+cu124. CUDA: 8.6. CUDA Toolkit: 12.4. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post3. FA2 = True]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: vLLM loading unsloth/gemma-2-2b-it-bnb-4bit with actual GPU utilization = 49.44%\n",
      "Unsloth: Your GPU has CUDA compute capability 8.6 with VRAM = 23.68 GB.\n",
      "Unsloth: Using conservativeness = 1.0. Chunked prefill tokens = 256. Num Sequences = 224.\n",
      "Unsloth: vLLM's KV Cache can use up to 9.25 GB. Also swap space = 6 GB.\n",
      "INFO 02-12 00:53:44 config.py:542] This model supports multiple tasks: {'generate', 'classify', 'embed', 'reward', 'score'}. Defaulting to 'generate'.\n",
      "Unsloth: vLLM Bitsandbytes config using kwargs = {'load_in_8bit': False, 'load_in_4bit': True, 'bnb_4bit_compute_dtype': 'bfloat16', 'bnb_4bit_quant_storage': 'uint8', 'bnb_4bit_quant_type': 'nf4', 'bnb_4bit_use_double_quant': True, 'llm_int8_enable_fp32_cpu_offload': False, 'llm_int8_has_fp16_weight': False, 'llm_int8_skip_modules': [], 'llm_int8_threshold': 6.0}\n",
      "INFO 02-12 00:53:45 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.2) with config: model='unsloth/gemma-2-2b-it-bnb-4bit', speculative_config=None, tokenizer='unsloth/gemma-2-2b-it-bnb-4bit', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=256, download_dir=None, load_format=bitsandbytes, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=bitsandbytes, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=unsloth/gemma-2-2b-it-bnb-4bit, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"level\":0,\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":224}, use_cached_outputs=False, \n",
      "INFO 02-12 00:53:46 cuda.py:230] Using Flash Attention backend.\n",
      "INFO 02-12 00:53:46 model_runner.py:1110] Starting to load model unsloth/gemma-2-2b-it-bnb-4bit...\n",
      "INFO 02-12 00:53:46 loader.py:1102] Loading weights with BitsAndBytes quantization.  May take a while ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W212 00:53:46.702426879 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-12 00:53:46 weight_utils.py:252] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2faaa5a7c8724690b33e3ab2c17c9ee3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1b97ccbb9e142df925110ea2322bf85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-12 00:53:48 model_runner.py:1115] Loading model weights took 2.1024 GB\n",
      "INFO 02-12 00:53:48 punica_selector.py:18] Using PunicaWrapperGPU.\n",
      "INFO 02-12 00:53:49 worker.py:267] Memory profiling takes 1.23 seconds\n",
      "INFO 02-12 00:53:49 worker.py:267] the current vLLM instance can use total_gpu_memory (23.68GiB) x gpu_memory_utilization (0.49) = 11.71GiB\n",
      "INFO 02-12 00:53:49 worker.py:267] model weights take 2.10GiB; non_torch_memory takes 0.06GiB; PyTorch activation peak memory takes 2.06GiB; the rest of the memory reserved for KV Cache is 7.49GiB.\n",
      "INFO 02-12 00:53:49 executor_base.py:110] # CUDA blocks: 4721, # CPU blocks: 3780\n",
      "INFO 02-12 00:53:49 executor_base.py:115] Maximum concurrency for 256 tokens per request: 295.06x\n",
      "INFO 02-12 00:53:52 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 31/31 [00:16<00:00,  1.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-12 00:54:08 model_runner.py:1562] Graph capturing finished in 16 secs, took 0.57 GiB\n",
      "INFO 02-12 00:54:08 llm_engine.py:431] init engine (profile, create kv cache, warmup model) took 20.75 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from unsloth import is_bfloat16_supported\n",
    "import torch\n",
    "max_seq_length = 256 # Can increase for longer reasoning traces\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"google/gemma-2-2b-it\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    load_in_4bit = True, # False for LoRA 16bit\n",
    "    fast_inference = True, # Enable vLLM fast inference\n",
    "    dtype = None,\n",
    "    gpu_memory_utilization = 0.5, # Reduce if out of memory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "31685d00-a834-4214-b6ed-ec97d1028693",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1319/1319 [22:49<00:00,  1.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of correct answers: 0.600\n",
      "Score of correct formats: 0.000\n",
      "[0.6004548900682335]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "accuracy_scores = []\n",
    "for k in range(1):\n",
    "    results = evaluate_model(model, tokenizer, test_data, runs=1, seed=k)\n",
    "    accuracy_scores.append(results[\"answer_accuracy\"])\n",
    "\n",
    "print(accuracy_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ef6ebb14-7c74-425c-8cb7-5d925a922d1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's how to solve the problem:\n",
      "\n",
      "* **White fiber:** The robe needs half as much white fiber as blue fiber, so it needs 2 bolts / 2 = 1 bolt of white fiber.\n",
      "* **Total fiber:**  The robe needs 2 bolts of blue + 1 bolt of white = 3 bolts of fiber in total. \n",
      "\n",
      "**Answer:** It takes a total of 3 bolts of fiber to make the robe. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "temperature=0.0\n",
    "top_p=0.95\n",
    "max_tokens=1024\n",
    "\n",
    "prompt = [{'role': 'user', 'content': test_data[1][\"question\"]}] \n",
    "text = tokenizer.apply_chat_template(prompt, tokenize = False, add_generation_prompt = True)\n",
    "    \n",
    "sampling_params = SamplingParams(\n",
    "    temperature = temperature,\n",
    "    top_p = top_p,\n",
    "    max_tokens = max_tokens,\n",
    ")\n",
    "\n",
    "output = model.fast_generate(\n",
    "    [text],\n",
    "    sampling_params = sampling_params,\n",
    "    use_tqdm=False,\n",
    ")[0].outputs[0].text\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "12d001ab-751b-4c7e-9957-bf355b06c742",
   "metadata": {},
   "outputs": [],
   "source": [
    "del(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "828bc3d2-c3b1-4df5-9e5b-89853df7147c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.2.5: Fast Gemma patching. Transformers: 4.48.3.\n",
      "   \\\\   /|    GPU: NVIDIA GeForce RTX 3090. Max memory: 23.684 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.1+cu124. CUDA: 8.6. CUDA Toolkit: 12.4. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post3. FA2 = True]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: vLLM loading unsloth/gemma-2b-it-bnb-4bit with actual GPU utilization = 27.25%\n",
      "Unsloth: Your GPU has CUDA compute capability 8.6 with VRAM = 23.68 GB.\n",
      "Unsloth: Using conservativeness = 1.0. Chunked prefill tokens = 256. Num Sequences = 192.\n",
      "Unsloth: vLLM's KV Cache can use up to 4.18 GB. Also swap space = 6 GB.\n",
      "INFO 02-12 01:17:20 config.py:542] This model supports multiple tasks: {'generate', 'classify', 'embed', 'reward', 'score'}. Defaulting to 'generate'.\n",
      "Unsloth: vLLM Bitsandbytes config using kwargs = {'load_in_8bit': False, 'load_in_4bit': True, 'bnb_4bit_compute_dtype': 'bfloat16', 'bnb_4bit_quant_storage': 'uint8', 'bnb_4bit_quant_type': 'nf4', 'bnb_4bit_use_double_quant': True, 'llm_int8_enable_fp32_cpu_offload': False, 'llm_int8_has_fp16_weight': False, 'llm_int8_skip_modules': [], 'llm_int8_threshold': 6.0}\n",
      "INFO 02-12 01:17:20 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.2) with config: model='unsloth/gemma-2b-it-bnb-4bit', speculative_config=None, tokenizer='unsloth/gemma-2b-it-bnb-4bit', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=256, download_dir=None, load_format=bitsandbytes, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=bitsandbytes, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=unsloth/gemma-2b-it-bnb-4bit, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"level\":0,\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":192}, use_cached_outputs=False, \n",
      "INFO 02-12 01:17:21 model_runner.py:1110] Starting to load model unsloth/gemma-2b-it-bnb-4bit...\n",
      "WARNING 02-12 01:17:21 gemma.py:60] Gemma's activation function was incorrectly set to exact GeLU in the config JSON file when it was initially released. Changing the activation function to approximate GeLU (`gelu_pytorch_tanh`). If you want to use the legacy `gelu`, edit the config JSON to set `hidden_activation=gelu` instead of `hidden_act`. See https://github.com/huggingface/transformers/pull/29402 for more details.\n",
      "INFO 02-12 01:17:21 loader.py:1102] Loading weights with BitsAndBytes quantization.  May take a while ...\n",
      "INFO 02-12 01:17:22 weight_utils.py:252] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc41e9047f2448b3a8dca843e2c4acf9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9aa429e145104b4dac20a74da3271b6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-12 01:17:23 model_runner.py:1115] Loading model weights took 1.9369 GB\n",
      "INFO 02-12 01:17:24 worker.py:267] Memory profiling takes 0.49 seconds\n",
      "INFO 02-12 01:17:24 worker.py:267] the current vLLM instance can use total_gpu_memory (23.68GiB) x gpu_memory_utilization (0.27) = 6.45GiB\n",
      "INFO 02-12 01:17:24 worker.py:267] model weights take 1.94GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.76GiB; the rest of the memory reserved for KV Cache is 2.76GiB.\n",
      "INFO 02-12 01:17:24 executor_base.py:110] # CUDA blocks: 10054, # CPU blocks: 21845\n",
      "INFO 02-12 01:17:24 executor_base.py:115] Maximum concurrency for 256 tokens per request: 628.38x\n",
      "INFO 02-12 01:17:27 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:13<00:00,  2.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-12 01:17:40 model_runner.py:1562] Graph capturing finished in 13 secs, took 0.13 GiB\n",
      "INFO 02-12 01:17:40 llm_engine.py:431] init engine (profile, create kv cache, warmup model) took 17.42 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Unsloth 2025.2.5 patched 18 layers with 18 QKV layers, 18 O layers and 18 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"grpo_gemma_saved_lora_2b\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    load_in_4bit = True, # False for LoRA 16bit\n",
    "    fast_inference = True, # Enable vLLM fast inference\n",
    "    dtype = None,\n",
    "    gpu_memory_utilization = 0.5, # Reduce if out of memory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200aaaec-1339-47a3-b2c0-30299260c6a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 7/1319 [00:02<09:07,  2.40it/s]"
     ]
    }
   ],
   "source": [
    "accuracy_scores = []\n",
    "for k in range(1):\n",
    "    results = evaluate_model(model, tokenizer, test_data, runs=1, seed=k)\n",
    "    accuracy_scores.append(results[\"answer_accuracy\"])\n",
    "\n",
    "print(accuracy_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651ace83-a652-4e43-bf3b-bfc679d55471",
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature=0.0\n",
    "top_p=0.95\n",
    "max_tokens=1024\n",
    "\n",
    "prompt = [{'role': 'user', 'content': test_data[1][\"question\"]}] \n",
    "text = tokenizer.apply_chat_template(prompt, tokenize = False, add_generation_prompt = True)\n",
    "    \n",
    "sampling_params = SamplingParams(\n",
    "    temperature = temperature,\n",
    "    top_p = top_p,\n",
    "    max_tokens = max_tokens,\n",
    ")\n",
    "\n",
    "output = model.fast_generate(\n",
    "    [text],\n",
    "    sampling_params = sampling_params,\n",
    "    use_tqdm=False,\n",
    ")[0].outputs[0].text\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7e8e5a-4562-4cd3-a14a-235783c18bbc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
