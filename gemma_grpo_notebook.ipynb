{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tuning Gemma-2-2B-IT with GRPO for Enhanced Reasoning\n\n",
    "This notebook provides a comprehensive pipeline for fine-tuning the `Google/gemma-2-2b-it` model using a two-stage process to enhance its mathematical reasoning capabilities, specifically targeting the GSM8K benchmark.\n\n",
    "## Project Overview\n\n",
    "The core of this project is a two-stage fine-tuning process:\n\n",
    "1.  **Supervised Fine-Tuning (SFT) on LIMO:** The model is first fine-tuned on the LIMO dataset. This step teaches the model to generate responses in a specific XML-like format, with a clear separation between the reasoning process and the final answer.\n\n",
    "2.  **Generative Reward Post-Optimization (GRPO) on GSM8K:** The SFT-tuned model is then further trained using GRPO on the GSM8K dataset. This reinforcement learning step optimizes the model to produce not only well-formatted but also correct answers to mathematical word problems.\n\n",
    "This approach combines the benefits of supervised learning for format adherence and reinforcement learning for improving the correctness of the generated solutions.\n\n",
    "### Key Technologies\n\n",
    "- **Parameter-Efficient Fine-Tuning (PEFT):** Utilizes LoRA for efficient training, reducing computational and memory requirements.\n",
    "- **vLLM Integration:** Leverages the vLLM library for high-throughput inference during GRPO, significantly speeding up the training process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration\n\n",
    "This section centralizes all the key configurations and utility functions for the project. It includes:\n\n",
    "- **System Prompts:** Definitions for the prompts used to guide the model's behavior.\n",
    "- **Configuration Class:** A `Config` class to hold static parameters like model names and sequence lengths.\n",
    "- **Utility Functions:** Helper functions for environment initialization, resource management (like closing vLLM), and checking hardware capabilities (e.g., bfloat16 support)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import warnings\n",
    "import contextlib\n",
    "import torch\n",
    "from huggingface_hub import login\n",
    "from datasets import load_dataset, Dataset\n",
    "from vllm.distributed.parallel_state import (\n",
    "    destroy_model_parallel,\n",
    "    destroy_distributed_environment,\n",
    ")\n",
    "\n",
    "# --- System Prompts and Formatting ---\n",
    "\n",
    "R1_STYLE_SYSTEM_PROMPT = \"\"\"A conversation between User and Assistant. The user asks a question, and the Assistant solves it.\\n",
    "The assistant first thinks about the reasoning process in the mind and then provides the user\\n",
    "with the answer.\"\"\"\n",
    "\n",
    "TASK_SPECIFIC_INSTRUCTIONS = \"\"\"The reasoning process and answer are enclosed within tags.The answer must be a single integer.\"\"\"\n",
    "\n",
    "EXAMPLE = \"\"\"<reasoning>\\n",
    "</reasoning>\\n",
    "<answer>\\n",
    "</answer>\"\"\"\n",
    "\n",
    "XML_COT_FORMAT = \"\"\"\\\n",
    "<reasoning>\\n",
    "{reasoning}\\n",
    "</reasoning>\\n",
    "<answer>\\n",
    "{answer}\\n",
    "</answer>\\n",
    "\"\"\"\n",
    "\n",
    "SYSTEM_PROMPT = (\n",
    "    R1_STYLE_SYSTEM_PROMPT + \"\\n\\n\" + TASK_SPECIFIC_INSTRUCTIONS + \"\\n\" + EXAMPLE + \"\\n\"\n",
    ")\n",
    "\n",
    "\n",
    "# --- Configuration Class ---\n",
    "\n",
    "class Config:\n",
    "    \"\"\"Configuration parameters\"\"\"\n",
    "\n",
    "    MODEL_NAME = \"Google/gemma-2-2b-it\"\n",
    "    OUTPUT_MODEL = \"gemma-2-2b-it-grpo\"\n",
    "\n",
    "    max_prompt_length = 256\n",
    "    max_completion_length = 256\n",
    "\n",
    "\n",
    "# --- Environment and Utility Functions ---\n",
    "\n",
    "def init():\n",
    "    \"\"\"Initialization script\"\"\"\n",
    "    os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"0\"\n",
    "    os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "    os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:128\"\n",
    "\n",
    "    # It is recommended to set the HF_TOKEN as an environment variable\n",
    "    token = os.environ.get(\"HF_TOKEN\")\n",
    "    if token:\n",
    "        login(token=token)\n",
    "    else:\n",
    "        print(\"HF_TOKEN not set. You might need to log in manually.\")\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "def close(llm=None):\n",
    "    \"\"\"Close vllm and clean up resources\"\"\"\n",
    "    destroy_model_parallel()\n",
    "    destroy_distributed_environment()\n",
    "    if llm:\n",
    "        del llm.llm_engine.model_executor\n",
    "        del llm\n",
    "    with contextlib.suppress(AssertionError):\n",
    "        torch.distributed.destroy_process_group()\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "def is_bfloat16_supported():\n",
    "    \"\"\"Checks if the current device supports bfloat16.\"\"\"\n",
    "    return torch.cuda.is_available() and torch.cuda.get_device_capability(0)[0] >= 8\n",
    "\n",
    "\n",
    "def info_device():\n",
    "    \"\"\"Get device for PyTorch\"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    return device\n",
    "\n",
    "# Note: The data loading functions will be moved to the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Preprocessing\n\n",
    "This section contains functions for loading and preprocessing the datasets used in this project:\n\n",
    "- **GSM8K:** A dataset of grade-school math word problems. The `get_gsm8k_questions` function loads the data and formats it into a prompt-answer structure.\n",
    "- **LIMO:** A dataset used for teaching the model the desired XML-like response format. The `get_limo` and `get_splitted_limo` functions handle loading and preparing this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_hash_answer(text: str) -> str | None:\n",
    "    \"\"\"Extract numeric answer from GSM8K example\"\"\"\n",
    "    if \"####\" not in text:\n",
    "        return None\n",
    "    return text.split(\"####\")[1].strip()\n",
    "\n",
    "\n",
    "def get_gsm8k_questions(split=\"train\"):\n",
    "    \"\"\"Upload GSM8k dataset\"\"\"\n",
    "    data = load_dataset(\"openai/gsm8k\", \"main\", cache_dir=\"/tmp\")[split]\n",
    "    data = data.map(\n",
    "        lambda x: {\n",
    "            \"prompt\": [\n",
    "                {\"role\": \"user\", \"content\": SYSTEM_PROMPT + \"\\n\" + x[\"question\"]},\n",
    "            ],\n",
    "            \"answer\": extract_hash_answer(x[\"answer\"]),\n",
    "        }\n",
    "    )\n",
    "    return data\n",
    "\n",
    "\n",
    "def get_limo(split=\"train\"):\n",
    "    \"\"\"Upload GAIR/LIMO dataset\"\"\"\n",
    "    data = load_dataset(\"GAIR/LIMO\", cache_dir=\"/tmp\")[split]\n",
    "    data = data.map(\n",
    "        lambda x: {\n",
    "            \"prompt\": \"\\n\".join(\n",
    "                [\n",
    "                    x[\"question\"],\n",
    "                    R1_STYLE_SYSTEM_PROMPT,\n",
    "                    \"<reasoning>\",\n",
    "                    x[\"solution\"],\n",
    "                    \"</reasoning>\",\n",
    "                    \"<answer>\",\n",
    "                    x[\"answer\"],\n",
    "                    \"<answer>\",\n",
    "                ]\n",
    "            )\n",
    "        }\n",
    "    )\n",
    "    return data\n",
    "\n",
    "\n",
    "def get_splitted_limo(tokenizer, split=\"train\", max_length=4096, overlap_chars=1024):\n",
    "    \"\"\"Load GAIR/LIMO dataset and split long texts into overlapping chunks of max_length tokens.\"\"\"\n",
    "\n",
    "    data = load_dataset(\"GAIR/LIMO\", cache_dir=\"/tmp\")[split]\n",
    "\n",
    "    examples = []\n",
    "    for example in data:\n",
    "\n",
    "        # Format text as a single string\n",
    "        text = \"\\n\".join(\n",
    "            [\n",
    "                example[\"question\"],\n",
    "                \"<reasoning>\",\n",
    "                example[\"solution\"],\n",
    "                \"</reasoning>\",\n",
    "                \"<answer>\",\n",
    "                example[\"answer\"],\n",
    "                \"</answer>\",\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Tokenize without truncation\n",
    "        input_ids = tokenizer.encode(text, truncation=False, add_special_tokens=False)\n",
    "\n",
    "        for i in range(0, len(input_ids), max_length - overlap_chars):\n",
    "            chunk = input_ids[i : i + max_length]\n",
    "            examples.append(tokenizer.decode(chunk))\n",
    "\n",
    "    dataset = Dataset.from_dict({\"prompt\": examples})\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading Demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's load a small sample of the GSM8K data to see its structure.\n",
    "gsm8k_sample = get_gsm8k_questions(split='test').select(range(2))\n",
    "\n",
    "for item in gsm8k_sample:\n",
    "    print(\"--- PROMPT ---\")\n",
    "    print(item['prompt'][0]['content'])\n",
    "    print(\"\\n--- GROUND TRUTH ANSWER ---\")\n",
    "    print(item['answer'])\n",
    "    print(\"=\"*30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Full Fine-Tuning Pipeline (SFT + GRPO)\n\n",
    "This section details the complete two-stage fine-tuning pipeline, which first trains the model to follow a specific format (SFT) and then optimizes it for correctness (GRPO).\n\n",
    "### 3.1. Helper and Reward Functions\n\n",
    "First, we define helper functions to extract answers from the model's XML-formatted output and the reward functions that will guide the GRPO training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from trl import GRPOConfig, GRPOTrainer, SFTConfig, SFTTrainer\n",
    "from peft import LoraConfig\n",
    "\n",
    "def extract_xml_answer(text, start_tag=\"<answer>\", end_tag=\"</answer>\"):\n",
    "    \"\"\"Extract the content within tags from a string using regex\"\"\"\n",
    "    pattern = re.escape(start_tag) + r\"(.*?)\" + re.escape(end_tag)\n",
    "    match = re.search(\n",
    "        pattern, text, re.DOTALL\n",
    "    )  # DOTALL allows matching across multiple lines\n",
    "\n",
    "    if match:\n",
    "        answer = match.group(1)\n",
    "        answer = re.sub(r\"[%$]\", \"\", answer).strip()  # Remove '%' and '$'\n",
    "        return answer\n",
    "\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "def extract_last_xml_answer(text, start_tag=\"<answer>\", end_tag=\"</answer>\"):\n",
    "    \"\"\"Extract the content within the last occurrence of tags from a string using regex\"\"\"\n",
    "    pattern = re.escape(start_tag) + r\"(.*?)\" + re.escape(end_tag)\n",
    "    matches = re.findall(pattern, text, re.DOTALL)  # Find all matches\n",
    "\n",
    "    if matches:\n",
    "        answer = matches[-1]  # Get the last match\n",
    "        answer = re.sub(r\"[%$]\", \"\", answer).strip()  # Remove '%' and '$'\n",
    "        return answer\n",
    "\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "def format_reward_func(completions, **kwargs):\n",
    "    \"\"\"Reward function that checks if the completion has the correct format.\"\"\"\n",
    "    pattern = r\"^<reasoning>[\\s\\S]*?<\\/reasoning>\\s*<answer>[\\s\\S]*?<\\/answer>$\"\n",
    "    responses = [completion[0][\"content\"] for completion in completions]\n",
    "    rewards = [1.0 if re.match(pattern, response) else 0.0 for response in responses]\n",
    "    return rewards\n",
    "\n",
    "\n",
    "def correctness_reward_func(completions, answer, **kwargs):\n",
    "    \"\"\"Reward function that checks if the answer is correct.\"\"\"\n",
    "    responses = [completion[0][\"content\"] for completion in completions]\n",
    "    extracted_responses = [extract_last_xml_answer(response) for response in responses]\n",
    "    rewards = [\n",
    "        2.0 if extracted == correct else 0.0\n",
    "        for extracted, correct in zip(extracted_responses, answer)\n",
    "    ]\n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. The SFT + GRPO Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_sft_and_grpo_pipeline(run_training=False):\n",
    "    \"\"\"Runs the full SFT and GRPO fine-tuning pipeline.\"\"\"\n",
    "    if not run_training:\n",
    "        print(\"Training is skipped. Set run_training=True to execute.\")\n",
    "        return\n",
    "    \n",
    "    init()\n",
    "    params = Config()\n",
    "\n",
    "    # --- PEFT Configuration (LoRA) ---\n",
    "    peft_config = LoraConfig(\n",
    "        r=64,\n",
    "        lora_alpha=64,\n",
    "        lora_dropout=0,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "        target_modules=[\n",
    "            \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "            \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "        ],\n",
    "    )\n",
    "    \n",
    "    # --- Stage 1: Supervised Fine-Tuning (SFT) on LIMO ---\n",
    "    print(\"\\n\" + \"*\"*16 + \" Stage 1: SFT Fine-Tuning on LIMO \" + \"*\"*16 + \"\\n\")\n",
    "    \n",
    "    sft_training_args = SFTConfig(\n",
    "        gradient_checkpointing=True,\n",
    "        gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
    "        fp16=not is_bfloat16_supported(),\n",
    "        bf16=is_bfloat16_supported(),\n",
    "        max_seq_length=4096, # Reduced from 8192/2 for stability\n",
    "        optim=\"adamw_8bit\",\n",
    "        learning_rate=2e-4,\n",
    "        weight_decay=0.01,\n",
    "        max_grad_norm=0.3,\n",
    "        lr_scheduler_type=\"linear\",\n",
    "        num_train_epochs=1,\n",
    "        warmup_ratio=0.05,\n",
    "        per_device_train_batch_size=1,\n",
    "        gradient_accumulation_steps=2,\n",
    "        remove_unused_columns=False,\n",
    "        packing=False,\n",
    "        report_to=\"tensorboard\",\n",
    "        logging_dir=\"logs/sft_runs\",\n",
    "        output_dir=\"sft_output\",\n",
    "        overwrite_output_dir=True,\n",
    "        seed=0,\n",
    "    )\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        params.MODEL_NAME,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        attn_implementation=\"eager\",\n",
    "        device_map=\"auto\",\n",
    "    )\n",
    "    model.config.use_cache = False\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(params.MODEL_NAME)\n",
    "    limo_train = get_limo(split=\"train\")\n",
    "\n",
    "    sft_trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        args=sft_training_args,\n",
    "        train_dataset=limo_train,\n",
    "        peft_config=peft_config,\n",
    "        formatting_func=lambda example: example[\"prompt\"],\n",
    "    )\n",
    "    \n",
    "    sft_trainer.train()\n",
    "    \n",
    "    # Clean up SFT resources\n",
    "    del sft_trainer, limo_train\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    # --- Stage 2: GRPO on GSM8K ---\n",
    "    print(\"\\n\" + \"*\"*16 + \" Stage 2: RL Fine-Tuning on GSM8K \" + \"*\"*16 + \"\\n\")\n",
    "\n",
    "    gsm8k_train = get_gsm8k_questions(split=\"train\")\n",
    "\n",
    "    grpo_training_args = GRPOConfig(\n",
    "        gradient_checkpointing=True,\n",
    "        gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
    "        fp16=not is_bfloat16_supported(),\n",
    "        bf16=is_bfloat16_supported(),\n",
    "        max_prompt_length=params.max_prompt_length,\n",
    "        max_completion_length=params.max_completion_length,\n",
    "        optim=\"adamw_8bit\",\n",
    "        learning_rate=1e-5,\n",
    "        weight_decay=0.1,\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        max_grad_norm=0.1,\n",
    "        warmup_ratio=0.05,\n",
    "        beta=0.005,  # Divergence coefficient\n",
    "        num_train_epochs=2,\n",
    "        per_device_train_batch_size=4,\n",
    "        gradient_accumulation_steps=4,\n",
    "        num_generations=4,\n",
    "        temperature=0.5,\n",
    "        use_vllm=True,\n",
    "        vllm_gpu_memory_utilization=0.35,\n",
    "        vllm_max_model_len=params.max_prompt_length + params.max_completion_length,\n",
    "        report_to=\"tensorboard\",\n",
    "        logging_dir=\"logs/grpo_runs\",\n",
    "        output_dir=\"grpo_output\",\n",
    "        overwrite_output_dir=True,\n",
    "    )\n",
    "\n",
    "    grpo_trainer = GRPOTrainer(\n",
    "        model=model,\n",
    "        processing_class=tokenizer,\n",
    "        reward_funcs=[correctness_reward_func, format_reward_func],\n",
    "        args=grpo_training_args,\n",
    "        train_dataset=gsm8k_train,\n",
    "        peft_config=peft_config,\n",
    "    )\n",
    "\n",
    "    grpo_trainer.train()\n",
    "\n",
    "    # --- Save Final Model ---\n",
    "    print(f\"\\nSaving final model to {params.OUTPUT_MODEL}\")\n",
    "    merged_model = grpo_trainer.model.merge_and_unload()\n",
    "    tokenizer.save_pretrained(params.OUTPUT_MODEL)\n",
    "    merged_model.save_pretrained(params.OUTPUT_MODEL)\n",
    "\n",
    "    close()\n",
    "\n",
    "# To run the training, call the function below.\n",
    "# Note: This requires a GPU and a configured environment.\n",
    "# run_sft_and_grpo_pipeline(run_training=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. GRPO-Only Fine-Tuning\n\n",
    "This section provides a simplified pipeline that runs only the GRPO training on the base model, skipping the initial SFT step. This can be useful for comparison or if the base model already has good format-following capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_grpo_only_pipeline(run_training=False):\n",
    "    \"\"\"Runs the GRPO-only fine-tuning pipeline.\"\"\"\n",
    "    if not run_training:\n",
    "        print(\"Training is skipped. Set run_training=True to execute.\")\n",
    "        return\n",
    "        \n",
    "    init()\n",
    "    params = Config()\n",
    "\n",
    "    gsm8k_train = get_gsm8k_questions(split=\"train\")\n",
    "\n",
    "    peft_config = LoraConfig(\n",
    "        r=64,\n",
    "        lora_alpha=64,\n",
    "        lora_dropout=0,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "        target_modules=[\n",
    "            \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "            \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    training_args = GRPOConfig(\n",
    "        use_vllm=True,\n",
    "        vllm_gpu_memory_utilization=0.35,\n",
    "        vllm_max_model_len=params.max_prompt_length + params.max_completion_length,\n",
    "        learning_rate=1e-5,\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        optim=\"adamw_8bit\",\n",
    "        bf16=is_bfloat16_supported(),\n",
    "        fp16=not is_bfloat16_supported(),\n",
    "        gradient_checkpointing=True,\n",
    "        gradient_accumulation_steps=4,\n",
    "        per_device_train_batch_size=4,\n",
    "        num_generations=4,\n",
    "        temperature=0.5,\n",
    "        max_prompt_length=params.max_prompt_length,\n",
    "        max_completion_length=params.max_completion_length,\n",
    "        num_train_epochs=2,\n",
    "        logging_steps=100,\n",
    "        save_steps=500,\n",
    "        max_grad_norm=0.1,\n",
    "        report_to=\"tensorboard\",\n",
    "        logging_dir=\"logs/grpo_only_runs\",\n",
    "        output_dir=\"grpo_only_output\",\n",
    "    )\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(params.MODEL_NAME)\n",
    "\n",
    "    trainer = GRPOTrainer(\n",
    "        model=params.MODEL_NAME,\n",
    "        processing_class=tokenizer,\n",
    "        reward_funcs=[correctness_reward_func, format_reward_func],\n",
    "        args=training_args,\n",
    "        train_dataset=gsm8k_train,\n",
    "        peft_config=peft_config,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    print(f\"\\nSaving final model to {params.OUTPUT_MODEL}_grpo_only\")\n",
    "    merged_model = trainer.model.merge_and_unload()\n",
    "    tokenizer.save_pretrained(f\"{params.OUTPUT_MODEL}_grpo_only\")\n",
    "    merged_model.save_pretrained(f\"{params.OUTPUT_MODEL}_grpo_only\")\n",
    "\n",
    "    close()\n",
    "\n",
    "# To run the training, call the function below.\n",
    "# Note: This requires a GPU and a configured environment.\n",
    "# run_grpo_only_pipeline(run_training=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluation\n\n",
    "This section provides the code to evaluate the fine-tuned model on the GSM8K test set. The evaluation script calculates several metrics:\n\n",
    "- **Format Correctness:** Whether the model's output adheres to the desired XML format.\n",
    "- **Plausibly Correct:** Whether the answer is correct based on a less strict extraction method.\n",
    "- **Correct:** Whether the answer within the `<answer>` tag is exactly correct.\n\n",
    "It also saves a detailed log of the evaluation in `records.json`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "def sampler(\n",
    "    model,\n",
    "    input_string,\n",
    "    temperature=0.0,\n",
    "    top_p=1.0,\n",
    "    max_prompt_length=None,\n",
    "    max_completion_length=256,\n",
    "):\n",
    "    \"\"\"LLM generation function using vLLM.\"\"\"\n",
    "    sampling_params = SamplingParams(\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        truncate_prompt_tokens=max_prompt_length,\n",
    "        max_tokens=max_completion_length,\n",
    "    )\n",
    "    output = model.generate([input_string], sampling_params, use_tqdm=False)\n",
    "    return output[0].outputs[0].text\n",
    "\n",
    "\n",
    "def find_number(search_string):\n",
    "    \"\"\"Finds the last number to appear in a string.\"\"\"\n",
    "    numbers = re.compile(\n",
    "        r\"-?[\d,]*\\.?\\d+\",\n",
    "        re.MULTILINE | re.DOTALL | re.IGNORECASE,\n",
    "    ).findall(search_string)\n",
    "    return numbers[-1] if numbers else \"\"\n",
    "\n",
    "\n",
    "def remove_symbols(x: str) -> str:\n",
    "    \"\"\"Remove commas, pct and USD symbols.\"\"\"\n",
    "    return x.replace(\",\", \"\").replace(\"%\", \"\").replace(\"$\", \"\").strip()\n",
    "\n",
    "\n",
    "def get_num_tokens(text, tokenizer_instance):\n",
    "    \"\"\"Count the number of tokens in a string of text.\"\"\"\n",
    "    encoding = tokenizer_instance(text, return_tensors=\"pt\")\n",
    "    return len(encoding[\"input_ids\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_evaluation(model_path, run_eval=False):\n",
    "    \"\"\"Runs the evaluation on the GSM8K test set.\"\"\"\n",
    "    if not run_eval:\n",
    "        print(\"Evaluation is skipped. Set run_eval=True to execute.\")\n",
    "        return\n",
    "\n",
    "    init()\n",
    "    params = Config()\n",
    "\n",
    "    print(f\"Evaluating model {model_path}\")\n",
    "    llm = LLM(model=model_path)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "    gsm8k_test = get_gsm8k_questions(\"test\")\n",
    "    records = {}\n",
    "    correct_format, plausibly_correct, correct = 0, 0, 0\n",
    "\n",
    "    for task_id, item in tqdm(enumerate(gsm8k_test), total=len(gsm8k_test)):\n",
    "        prompt = item[\"prompt\"][0][\"content\"]\n",
    "        ground_truth_answer = item[\"answer\"]\n",
    "\n",
    "        response = sampler(\n",
    "            llm,\n",
    "            input_string=prompt,\n",
    "            temperature=0,\n",
    "            max_prompt_length=params.max_prompt_length,\n",
    "            max_completion_length=params.max_completion_length,\n",
    "        )\n",
    "\n",
    "        # Check format\n",
    "        pattern = r\"^<reasoning>[\\s\\S]*?<\\/reasoning>\\s*<answer>[\\s\\S]*?<\\/answer>$\"\n",
    "        if re.match(pattern, response.strip()):\n",
    "            correct_format += 1\n",
    "\n",
    "        # Check correctness\n",
    "        last_numeric_response = remove_symbols(find_number(response))\n",
    "        extracted_xml_answer = extract_last_xml_answer(response)\n",
    "\n",
    "        if last_numeric_response == ground_truth_answer or extracted_xml_answer == ground_truth_answer:\n",
    "            plausibly_correct += 1\n",
    "        \n",
    "        if extracted_xml_answer == ground_truth_answer:\n",
    "            correct += 1\n",
    "\n",
    "        records[task_id] = {\n",
    "            \"prompt\": prompt,\n",
    "            \"answer\": ground_truth_answer,\n",
    "            \"response\": response,\n",
    "            \"last_numeric_response\": last_numeric_response,\n",
    "            \"xml_response\": extracted_xml_answer,\n",
    "            \"xml_match\": bool(re.match(pattern, response.strip())),\n",
    "        }\n",
    "\n",
    "    with open(\"records.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(records, f, indent=4)\n",
    "\n",
    "    total = len(gsm8k_test)\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"Correct format:       {correct_format} / {total} ({correct_format / total:.2%})\")\n",
    "    print(f\"Plausibly correct:    {plausibly_correct} / {total} ({plausibly_correct / total:.2%})\")\n",
    "    print(f\"Correct:              {correct} / {total} ({correct / total:.2%})\")\n",
    "    print(\"=\" * 40)\n",
    "\n",
    "    close(llm)\n",
    "\n",
    "# To run evaluation on the fully fine-tuned model:\n",
    "# run_evaluation(Config.OUTPUT_MODEL, run_eval=True)\n",
    "\n",
    "# To run evaluation on the GRPO-only model:\n",
    "# run_evaluation(f\"{Config.OUTPUT_MODEL}_grpo_only\", run_eval=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Code Improvements and Suggestions\n\n",
    "While the provided scripts are functional and well-structured, here are some suggestions for further improvement:\n\n",
    "1.  **Refactor Shared Functions into a Utility Module:**\n",
    "    - The functions `extract_xml_answer` and `extract_last_xml_answer` are duplicated across multiple scripts. They could be moved into the `config.py` file or a new `utils.py` module to avoid code repetition (Don't Repeat Yourself - DRY principle).\n\n",
    "2.  **Use a Command-Line Argument Parser:**\n",
    "    - Instead of having the main logic run directly in `if __name__ == \"__main__\":`, consider using a library like `argparse` or `typer`. This would allow for more flexible execution, such as specifying the model name, output directories, or training parameters directly from the command line without editing the code.\n\n",
    "3.  **Implement Unit Tests:**\n",
    "    - The helper functions, especially those for data extraction and manipulation (e.g., `extract_xml_answer`, `find_number`), would benefit from unit tests. This would ensure they work correctly and prevent regressions if they are modified in the future. A testing framework like `pytest` would be suitable.\n\n",
    "4.  **Structured Logging:**\n",
    "    - Replace `print()` statements with a structured logging library like `logging`. This provides more control over log levels (e.g., INFO, DEBUG, ERROR), allows for easy redirection of logs to files, and makes the output more machine-readable for monitoring.\n\n",
    "5.  **Configuration Management:**\n",
    "    - For larger projects, managing configurations in a Python file can become cumbersome. Using a dedicated configuration format like YAML or TOML, and loading it with a library like `hydra`, could provide more flexibility and better organization."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
